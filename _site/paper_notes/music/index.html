<!DOCTYPE html>
<html>

<head>
  <title>Salted Fish Routine</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="I am currently a Research Assistant with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China. Before that, I received the M.Sc degree from the Hong Kong Baptist University. My research interests includes natural language processing and computer music.">
    <meta property="og:description" content="I am currently a Research Assistant with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China. Before that, I received the M.Sc degree from the Hong Kong Baptist University. My research interests includes natural language processing and computer music." />
    
    <meta name="author" content="Xiaoyin Li, Catherine" />

    
    <meta property="og:title" content="Paper notes" />
    <meta property="twitter:title" content="Paper notes" />
    

  <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="stylesheet" type="text/css" href="/style.css" />
  <!-- <link rel="alternate" type="application/rss+xml" title="Xiaoyin Li, Catherine - I am currently a Research Assistant with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China. Before that, I received the M.Sc degree from the Hong Kong Baptist University. My research interests includes natural language processing and computer music." href="/feed.xml" /> -->

  <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  <style>
    #chart-container {
      /* position: relative; */
      height: 400px;
      overflow: hidden;
    }
  </style>
</head>

<body>
  <div id="main" role="main" class="container" style="max-width: 80%">
    <h1>Paper notes</h1>
    <h3> </h3>

    <div id="chart-container"></div>

    <h2 id="1-gonzalez-manuel-f-and-john-r-aiello-more-than-meets-the-ear-investigating-how-music-affects-cognitive-task-performance-journal-of-experimental-psychology-applied-253-2019-431">1. Gonzalez, Manuel F., and John R. Aiello. <strong>“More than meets the ear: Investigating how music affects cognitive task performance.”</strong> Journal of Experimental Psychology: Applied 25.3 (2019): 431.</h2>

<p>This paper explores how music affects task performance based on three factors: the type of music (simple or complex), the task complexity (simple or complex), and the listener’s preference for external stimulation (introvert or extravert). Results show that music generally impairs complex task performance but complex music aids simple tasks. Preferences for external stimulation influence these effects: introverts benefit from music during complex tasks, while extraverts experience impaired performance, particularly with complex tasks. Volume also plays a role, with introverts performing better with soft music and extraverts with loud music on simple tasks.</p>

<hr />

<ul>
  <li>This paper investigate how music affects task performance in three different ways:
    <ul>
      <li>a) What one is listening to (simple music or complex music, determined by the number of instruments; more instruments make it more complex).</li>
      <li>b) what task is being performed (simple task or complex task)</li>
      <li>c) who is listening to the music (low [introvert] or high [extravert] preference for external stimulation). Participants completed personality measures, including the Boredom Proneness Scale, to determine their preference for external stimulation.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>The results suggest that:
    <ul>
      <li>In general:
        <ul>
          <li><strong>Music impaired complex task performacne</strong>.</li>
          <li><strong>Complex music facilitated simple task performance</strong>.</li>
        </ul>
      </li>
      <li>However, perference for external stimulation moderated these effects,
        <ul>
          <li><strong>For low preference for external stimulation</strong>:
            <ul>
              <li>Simple task:
                <ul>
                  <li>complex music facilitated simple task performance, with no score difference between no music and simple music.</li>
                  <li>Soft volume of music (50-56dB) facilitated simple task performacne the most, followed by loud volume (62-78dB), and the least was no music.</li>
                </ul>
              </li>
              <li>Complex task:
                <ul>
                  <li>Muisc facilitated complex task performance compare to no music.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong>For high preference for external stimulation</strong>:
            <ul>
              <li>Simple task:
                <ul>
                  <li>Simple music slightly facilitated simple task performance compared to no music, while complex music impaired performance.</li>
                  <li>Loud volume of music (62-78dB) facilitated task performance more than no music; soft music (50-56dB) impaired task performance.</li>
                </ul>
              </li>
              <li>Complex task:
                <ul>
                  <li>Music harmed task performance significantly.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="2-benetos-emmanouil-et-al-automatic-music-transcription-an-overview-ieee-signal-processing-magazine-361-2018-20-30">2. Benetos, Emmanouil, et al. “Automatic music transcription: An overview.” IEEE Signal Processing Magazine 36.1 (2018): 20-30.</h2>

<p>This paper focuses on the transcription of polyphonic music—a complex mixture of multiple simultaneous sound sources from pitched instruments and voices. Automatic Music Transcription (AMT) is recognized as a transformative technology with significant potential for both economic and societal benefits, including:</p>
<ul>
  <li><strong>Music Education</strong>: Facilitating automatic instrument tutoring.</li>
  <li><strong>Music Creation</strong>: Capturing improvisations and providing automatic accompaniments.</li>
  <li><strong>Music Production</strong>: Enhancing content visualization and editing capabilities.</li>
  <li><strong>Music Search and Musicology</strong>: Enabling indexing based on specific musical elements like melody or rhythm and detailed analysis of nonnotated music, such as jazz improvisations.</li>
</ul>

<hr />
<p><strong>Challenges and Open Problems in AMT</strong>:</p>
<ul>
  <li>Inffering musical attributes (e.g., pitch) from the mixture signal is an extremely underdetermined problem.</li>
  <li>For NN-based approaches:
    <ul>
      <li>Only a few, small annotated data sets available, and these are often subject to severe biases.</li>
      <li>Adaptability to new acoustic conditions.</li>
    </ul>
  </li>
  <li>Creation of context-specific AMT systems for multiple instruments.(such as given prior knowledge music style, instrument, …)</li>
  <li>The discrepancies in musical assumptions between Western and non-Western traditions, such as variations in octave frequency ranges and the limited availability of non-Western music data.</li>
  <li>Creation of a robust music transcription system that supports both pitched and unpited sound.</li>
  <li>The existing metrics for evaluating music transcription do not always align with human musical perception. For example, the presence of an extra note can be a more critical error than a missing note, and out-of-tone errors may be penalized more severely than in-tone ones.
    <ul>
      <li>❗ <strong>Lack of perceptually relevant evaluation metrics</strong> ❗ for AMT and the creation of evaluation metrics for notation-level transcription.</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Basic Knowledge</strong>:</p>

<p>Musical note is usually characterized by three elements: pitch, onset time, and offset time.
THe beginning of a note(or attack phase) might have entirely different spectral properties than the central part(decay phase). How to model uch relationship?</p>

  </div>

  <div class="wrapper-footer">
    <div class="container">
      <footer class="footer">
        












      </footer>
    </div>
  </div>

  

</body>

<script src="https://registry.npmmirror.com/echarts/5.5.1/files/dist/echarts.min.js"></script>
<script>
  const dom = document.getElementById('chart-container');

  const myChart = echarts.init(dom, null, {
    renderer: 'canvas',
    useDirtyRect: false
  });
  var app = {};

  var option;
  const max_value = 60;
  const min_value = 45;

  const data = [
    ["2024-10-01", 57.4, '-', 0],
    ["2024-10-02", '-', 51, 0],
    ["2024-10-04", '-', 37, 0],
    ["2024-10-05", 57.2, 46, 0],
    ["2024-10-06", 56.8, 40, 0],
    ["2024-10-07", 56.6, 46, 0],
    ["2024-10-08", 56.8, 40, 0],
    ["2024-10-10", '-', '-', 0],
    ["2024-10-15", '-', '-', 2000],
    ["2024-10-16", 57.5, 45, 1000],
    ["2024-10-17", 57.2, '-', 3000],
    ["2024-10-18", 57.0, '-', 1000],
    ["2024-10-19", 57.5, '-', 2500],
    ["2024-10-20", 56.2, 45, 1300],
    ["2024-10-21", 56.8, '-', 1500],
    ["2024-10-22", 57.5, '-', 1500],
    ["2024-10-23", '-', '-', '-'],
    ["2024-10-29", 56.5, '-', 1000],
    ["2024-10-30", 57.2, '-', 1000],
  ]

  const dateList = data.map(function (item) {
    return item[0];
  });
  const valueList = data.map(function (item) {
    return item[1];
  });
  const fitnessList = data.map(function (item) {
    return item[2];
  });


  const waterList = data.map(function (item) {
    if (item[3] == 0) {
      return 0
    }

    return item[3] / 2000 * (max_value - min_value) + min_value;
  });



  option = {
    // Make gradient line here
    visualMap: [
      {
        show: false,
        type: 'continuous',
        seriesIndex: 0,
        min: min_value,
        max: max_value
      },
    ],
    title: [
      {
        left: 'center',
        text: 'Body Chart'
      }
    ],
    tooltip: {
      trigger: 'axis',
      axisPointer: { type: 'shadow' },
      // formatter: function (params) {

      //   return `${params.seriesName} <br />
      //         ${params.name}: ${params.data.value} (${params.percent}%)<br />
      //         ${params.data.name1}: ${params.data.value1}`;
      // }
    },
    xAxis:
    {
      data: dateList
    },
    yAxis:
    {
      type: 'value',
      min: 35,
      max: 58,
      // axisLabel: {
      //   formatter: '{value} '
      // }
    },
    grid: [
      {
        left: '0%'
      },
      {
        left: '80%'
      }
    ],
    series: [
      {
        type: 'line',
        name: 'Weight',
        showSymbol: true,
        symbol: 'circle',
        connectNulls: true,
        symbolSize: 10,
        smooth: true,
        data: valueList,
        detail: {
          valueAnimation: true,
          formatter: '{value} kg',
          color: 'inherit'
        },
      },
      {
        name: 'Exercise Time',
        type: 'line',
        showSymbol: true,
        symbol: 'circle',
        connectNulls: true,
        symbolSize: 10,
        smooth: true,
        data: fitnessList,
        detail: {
          valueAnimation: true,
          formatter: '{value} mins',
          color: 'inherit'
        },
      },
      {
        name: 'Water',
        type: 'bar',
        data: waterList,
        detail: {
          valueAnimation: true,
          formatter: '{value} %',
          color: 'inherit'
        },
      },
    ]

  };

  if (option && typeof option === 'object') {
    myChart.setOption(option);
  }

  window.addEventListener('resize', myChart.resize);


  // const chartDom2 = document.getElementById('water');


  // const myChart2 = echarts.init(chartDom2, null, {
  //   renderer: 'canvas',
  //   useDirtyRect: false
  // });

  // var option2;

  // option2 = {
  //   series: [
  // {
  //   type: 'gauge',
  //   axisLine: {
  //     lineStyle: {
  //       width: 30,
  //       color: [
  //         [0.3, '#67e0e3'],
  //         [0.7, '#37a2da'],
  //         [1, '#fd666d']
  //       ]
  //     }
  //   },
  //   pointer: {
  //     itemStyle: {
  //       color: 'auto'
  //     }
  //   },
  //   axisTick: {
  //     distance: -30,
  //     length: 8,
  //     lineStyle: {
  //       color: '#fff',
  //       width: 2
  //     }
  //   },
  //   splitLine: {
  //     distance: -30,
  //     length: 30,
  //     lineStyle: {
  //       color: '#fff',
  //       width: 4
  //     }
  //   },
  //   axisLabel: {
  //     color: 'inherit',
  //     distance: 40,
  //     fontSize: 20
  //   },
  //   detail: {
  //     valueAnimation: true,
  //     formatter: '{value} ml',
  //     color: 'inherit'
  //   },
  //   data: [
  //     {
  //       value: 70
  //     }
  //   ]
  // }
  //   ]
  // };

  // option2 && myChart2.setOption(option2);

  // window.addEventListener('resize', myChart2.resize);


</script>

</html>